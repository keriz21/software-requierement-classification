{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEMBERSIHAN KATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Hapus karakter asing menggunakan ekspresi reguler\n",
    "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    cleaned_text.lower()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"binary.csv\")\n",
    "df.describe()\n",
    "\n",
    "requirement = df['Requirement']\n",
    "type = df['Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = requirement.apply(clean_text)\n",
    "requirement.to_csv(\"test13.csv\", index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "penghapusan stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Tokenisasi teks\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Ambil stop words dari NLTK dan tambahkan tanda baca ke dalamnya\n",
    "    stop_words = set(stopwords.words(\"english\") + list(string.punctuation))\n",
    "\n",
    "    # Hapus stop words dari token\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Gabungkan kembali token menjadi teks\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      feature application shall provide optimized ro...\n",
      "1      feature provides user select optimized route a...\n",
      "2      Fast internet connection mandatory would incre...\n",
      "3              every user request new maps needed loaded\n",
      "4      Google Maps API used application mandatory abi...\n",
      "                             ...                        \n",
      "417             designated phone number users send texts\n",
      "418    Texts sent number sent API system reply user a...\n",
      "419    question understood API system send text conta...\n",
      "420    Upon USB plugged system shall able deployed op...\n",
      "421    system shall able handle 1000 customers logged...\n",
      "Name: Requirement, Length: 422, dtype: object\n"
     ]
    }
   ],
   "source": [
    "requirement = requirement.apply(remove_stopwords)\n",
    "print(requirement)\n",
    "\n",
    "requirement.to_csv(\"text_word123.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (397, 0)\t1.0\n",
      "  (398, 0)\t1.0\n",
      "  (399, 0)\t1.0\n",
      "  (400, 0)\t1.0\n",
      "  (401, 0)\t1.0\n",
      "  (402, 0)\t1.0\n",
      "  (403, 0)\t1.0\n",
      "  (404, 0)\t1.0\n",
      "  (405, 0)\t1.0\n",
      "  (406, 0)\t1.0\n",
      "  (407, 0)\t1.0\n",
      "  (408, 0)\t1.0\n",
      "  (409, 0)\t1.0\n",
      "  (410, 0)\t1.0\n",
      "  (411, 0)\t1.0\n",
      "  (412, 0)\t1.0\n",
      "  (413, 0)\t1.0\n",
      "  (414, 0)\t1.0\n",
      "  (415, 0)\t1.0\n",
      "  (416, 0)\t1.0\n",
      "  (417, 0)\t1.0\n",
      "  (418, 0)\t1.0\n",
      "  (419, 0)\t1.0\n",
      "  (420, 0)\t1.0\n",
      "  (421, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(requirement)\n",
    "y = vectorizer.fit_transform(type)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.84      0.99      0.90        67\n",
      "         NFR       0.83      0.28      0.42        18\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.83      0.63      0.66        85\n",
      "weighted avg       0.83      0.84      0.80        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# Pastikan variabel X dan y sudah terdefinisi\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, type, test_size=0.2)\n",
    "\n",
    "# Coba latih model kembali setelah memastikan semua hal di atas\n",
    "# model_SVC.fit(X_train, y_train)\n",
    "\n",
    "model_SVC = SVC()\n",
    "model_SVC.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = model_SVC.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.92      0.92      0.92        61\n",
      "         NFR       0.79      0.79      0.79        24\n",
      "\n",
      "    accuracy                           0.88        85\n",
      "   macro avg       0.85      0.85      0.85        85\n",
      "weighted avg       0.88      0.88      0.88        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,type,test_size= 0.2)\n",
    "\n",
    "model_DT = DecisionTreeClassifier()\n",
    "model_DT.fit(X_train, y_train)\n",
    "\n",
    "predictions = model_DT.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.83      1.00      0.91        63\n",
      "         NFR       1.00      0.41      0.58        22\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.91      0.70      0.74        85\n",
      "weighted avg       0.87      0.85      0.82        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,type,test_size= 0.2)\n",
    "\n",
    "model_NB = MultinomialNB()\n",
    "model_NB.fit(X_train, y_train)\n",
    "\n",
    "predictions = model_NB.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.93      0.97      0.95        67\n",
      "         NFR       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.92        85\n",
      "   macro avg       0.90      0.85      0.87        85\n",
      "weighted avg       0.92      0.92      0.91        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,type,test_size= 0.2)\n",
    "\n",
    "model_MLP = MLPClassifier(hidden_layer_sizes=(4,), max_iter=1000, random_state=42)\n",
    "model_MLP.fit(X_train, y_train)\n",
    "\n",
    "predictions = model_MLP.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.74      1.00      0.85        59\n",
      "         NFR       1.00      0.19      0.32        26\n",
      "\n",
      "    accuracy                           0.75        85\n",
      "   macro avg       0.87      0.60      0.59        85\n",
      "weighted avg       0.82      0.75      0.69        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,type,test_size= 0.2)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_RF.fit(X_train, y_train)\n",
    "\n",
    "predictions = model_RF.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FR       0.83      1.00      0.91        70\n",
      "         NFR       1.00      0.07      0.12        15\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.92      0.53      0.52        85\n",
      "weighted avg       0.86      0.84      0.77        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,type,test_size= 0.2)\n",
    "\n",
    "model_LR = LogisticRegression(random_state=42)\n",
    "\n",
    "model_LR.fit(X_train,y_train)\n",
    "\n",
    "predictions = model_LR.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
